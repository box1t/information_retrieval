
# Добыча корпуса документов

## Принцип работы **`lab1`**

1. **Потоковая обработка.** Использован метод `ET.iterparse`. Он позволяет обрабатывать XML-документ по мере чтения, не загружая весь файл в ОЗУ, извлекая только необходимые узлы.
   
2. **Очистка от пространств имен.** Реализована логика динамической фильтрации XML-пространств (namespaces) через метод:
   $\text{tag} = \text{element.tag.split('\}')[-1]}$ 
   для обеспечения универсального поиска элементов.

3. **Извлечение контента.** Для каждого найденного блока документа `<page>` выполняется обход вложенных элементов для поиска иерархического тега `<text>`.

4. **Оптимизация памяти.** После обработки каждого элемента вызывается принудительная очистка ссылок на узлы дерева XML:
   $elem.clear() \implies \text{Garbage Collector free memory}$

5. **Статистический анализ.** Проводится сопоставление размера исходного файла с объемом извлеченного текста. Расчет математического ожидания длины текста $E[L]$ производится по формуле:
   $$E[L] = \frac{1}{n} \sum_{i=1}^{n} len(text_i)$$
   где $n$ — общее количество обработанных страниц.

## План тестирования

```sh
tail -n 20 # прочитать последнюю строку файла - целостность файла
```

## Запуск модуля

1. Перейдем на сайты и указанные страницы service/statistics
2. загрузим дампы бд
3. `sudo apt update && sudo apt install p7zip-full -y`
4. `7z x .xml.7z`
5. перенос файла из downloads в текущую директорию
6. chmod +r src/lab1/rustalker_pages_current.xml
7. исполнить скрипт с указанием директории в последней строке к xml файлу `python3 src/lab1/analyze_wiki_dump.py`


